{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirans1ngh/Machine-Learning-practice/blob/main/Gradient_Descent_Single_Step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate controls how big of a step we take\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Our sample data points\n",
        "x_data = [1, 2, 3, 4]\n",
        "y_data = [3, 5, 6, 7]\n",
        "\n",
        "# Initial guess for our line's parameters (y = theta1*x + theta0)\n",
        "# We start with a flat line at y=0\n",
        "theta0 = 0\n",
        "theta1 = 0\n",
        "\n",
        "# --- Calculation ---\n",
        "\n",
        "# Variables to hold the sum of our calculations\n",
        "sum_error = 0\n",
        "sum_squared_error = 0\n",
        "sum_error_times_x = 0\n",
        "\n",
        "# Go through each data point\n",
        "for i in range(len(x_data)):\n",
        "    # The current x and y value\n",
        "    x = x_data[i]\n",
        "    y = y_data[i]\n",
        "\n",
        "    # 1. Make a prediction using our current line formula\n",
        "    prediction = theta0 + theta1 * x\n",
        "\n",
        "    # 2. Calculate the error (how far off was the prediction?)\n",
        "    error = prediction - y\n",
        "\n",
        "    # 3. Add the results to our running totals\n",
        "    sum_error += error\n",
        "    sum_squared_error += error ** 2  # Same as error * error\n",
        "    sum_error_times_x += error * x\n",
        "\n",
        "# --- Final Calculations (Averages) ---\n",
        "\n",
        "# The number of data points\n",
        "m = len(x_data)\n",
        "\n",
        "# Cost (J): The average squared error. A measure of how bad our line is.\n",
        "cost = sum_squared_error / (2 * m)\n",
        "\n",
        "# Gradient for theta0: Average error. Tells us how to change the y-intercept.\n",
        "gradient_theta0 = sum_error / m\n",
        "\n",
        "# Gradient for theta1: Average of (error * x). Tells us how to change the slope.\n",
        "gradient_theta1 = sum_error_times_x / m\n",
        "\n",
        "\n",
        "# --- Update Parameters ---\n",
        "\n",
        "# Nudge theta0 in the right direction to reduce the error\n",
        "updated_theta0 = theta0 - learning_rate * gradient_theta0\n",
        "\n",
        "# Nudge theta1 in the right direction to reduce the error\n",
        "updated_theta1 = theta1 - learning_rate * gradient_theta1\n",
        "\n",
        "\n",
        "# --- Output the results ---\n",
        "print(f\"Number of data points (m): {m}\\n\")\n",
        "\n",
        "print(\"--- Intermediate Sums ---\")\n",
        "print(f\"Sum of Errors: {sum_error:.2f}\")\n",
        "print(f\"Sum of Squared Errors: {sum_squared_error:.2f}\")\n",
        "print(f\"Sum of (Error * x): {sum_error_times_x:.2f}\\n\")\n",
        "\n",
        "print(\"--- Cost and Gradients ---\")\n",
        "print(f\"Cost (J): {cost:.4f}\")\n",
        "print(f\"Gradient for theta0: {gradient_theta0:.4f}\")\n",
        "print(f\"Gradient for theta1: {gradient_theta1:.4f}\\n\")\n",
        "\n",
        "print(\"--- Updated Parameters ---\")\n",
        "print(f\"New theta0: {updated_theta0:.4f}\")\n",
        "print(f\"New theta1: {updated_theta1:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FcB8t5T98i3T"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}